{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <u>Data Mining</u>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After checking multiple websites and their recipes format we decided to scrape alllrecipes.com."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function scraping a singular recipe page.</br>\n",
    "The function receives url of a single recipe page and a selenium google driver.</br>\n",
    "It returns a list of: recipe, date(list), rating, raters, time(overall time in minutes), categories(list), servings, ingredients(list), instructions(list), calories, fat, carbs, proteins.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SingleRecipePageScrape(url, driver):\n",
    "    # Setting up the driver\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        driver.maximize_window()\n",
    "        \n",
    "    except:\n",
    "        print(\"Failed to load the page: \" + url)\n",
    "        return\n",
    "    \n",
    "    driver.implicitly_wait(5)\n",
    "\n",
    "    # Scraping the data from a script containing scheme with the recipe's info\n",
    "    try:\n",
    "        html_content = driver.page_source\n",
    "        soup = BeautifulSoup(html_content,'html.parser')\n",
    "        data_dict = json.loads(soup.find(\"script\", {\"id\" : \"allrecipes-schema_1-0\"}).text)[0]\n",
    "    except:\n",
    "        driver.close()\n",
    "        return\n",
    "    \n",
    "    # Name\n",
    "    try:\n",
    "        recipe = data_dict[\"name\"]\n",
    "    except:\n",
    "        recipe = None\n",
    "\n",
    "    # Date\n",
    "    try:\n",
    "        date_str = data_dict[\"datePublished\"]\n",
    "        date_list = []\n",
    "        date_list.append(int(date_str[:4]))\n",
    "        date_list.append(int(date_str[5:7]))\n",
    "    except:\n",
    "        date_list = None\n",
    "\n",
    "    # Rating\n",
    "    try:\n",
    "        rating = float(data_dict[\"aggregateRating\"][\"ratingValue\"])\n",
    "    except:\n",
    "        rating = None\n",
    "\n",
    "    # Raters\n",
    "    try:\n",
    "        raters = int(data_dict[\"aggregateRating\"][\"ratingCount\"])\n",
    "    except:\n",
    "        raters = None\n",
    "\n",
    "    # Time in minutes\n",
    "    try:\n",
    "        time_cook = int(data_dict[\"totalTime\"][2:-1])\n",
    "    except:\n",
    "        time_cook = None\n",
    "\n",
    "    # Category\n",
    "    try:\n",
    "        category_list = data_dict[\"recipeCategory\"]\n",
    "    except:\n",
    "        category_list = None\n",
    "\n",
    "    # Servings\n",
    "    try:\n",
    "        servings = int(data_dict[\"recipeYield\"][0])\n",
    "    except:\n",
    "        servings = None\n",
    "\n",
    "    # Ingredients, using the html file itself\n",
    "    try:\n",
    "        ingredients_spans = soup.find_all(\"span\", {\"data-ingredient-name\" : \"true\"})\n",
    "        ingredients_list = []\n",
    "        for span in ingredients_spans:\n",
    "            ingredients_list.append(span.text)\n",
    "    except:\n",
    "        ingredients_list = None\n",
    "\n",
    "    # Instructions\n",
    "    try:\n",
    "        instructions = data_dict[\"recipeInstructions\"]\n",
    "        instructions_list = []\n",
    "        for ins in instructions:\n",
    "            instructions_list.append(ins[\"text\"])\n",
    "    except:\n",
    "        instructions_list = None\n",
    "        \n",
    "    # Nutrition, using the html file itself\n",
    "    try:\n",
    "        nutrition_td = soup.find_all(\"td\",{\"class\": \"mntl-nutrition-facts-summary__table-cell type--dog-bold\"})\n",
    "        nutrition_list = []\n",
    "        for nutrition in nutrition_td:\n",
    "            nutrition_list.append(re.sub(r'[^0-9.]','',nutrition.text))\n",
    "    \n",
    "        calories = float(nutrition_list[0])\n",
    "        fat = float(nutrition_list[1])\n",
    "        carbs = float(nutrition_list[2])\n",
    "        proteins = float(nutrition_list[3])\n",
    "    except:\n",
    "        calories = None\n",
    "        fat = None\n",
    "        carbs = None\n",
    "        proteins = None\n",
    "\n",
    "    values = [recipe, date_list, rating, raters, time_cook, category_list, servings, ingredients_list, instructions_list, calories, fat, carbs, proteins]\n",
    "    return values\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function scraping a topic page with multiple recipes in it.</br>\n",
    "The function receives a url of topic page and selenium google driver</br>\n",
    "It returns list of recipes urls in the topic page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MultipleRecipesPageScrape(url, driver):\n",
    "    # Setting up the driver\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        driver.maximize_window()\n",
    "        \n",
    "    except:\n",
    "        print(\"Failed to load the multiple recipes page: \" + url)\n",
    "        return\n",
    "\n",
    "    driver.implicitly_wait(5)\n",
    "\n",
    "    # Scraping the recipes from topic page\n",
    "    try:\n",
    "        html_content = driver.page_source\n",
    "        soup = BeautifulSoup(html_content,'html.parser')\n",
    "        data_list = json.loads(soup.find(\"script\", {\"id\" : \"allrecipes-schema_1-0\"}).text)[0][\"itemListElement\"]\n",
    "        recipes_links = []\n",
    "    except:\n",
    "        driver.close()\n",
    "        return\n",
    "    for item in data_list:\n",
    "        try:\n",
    "            recipes_links.append(item[\"url\"])\n",
    "        except:\n",
    "            continue\n",
    "    return recipes_links\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function scraping the allrecipes website index.</br>\n",
    "The function recives google selenium driver.</br>\n",
    "It returns the list of all topics in the index page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TopicPageScrape(driver):\n",
    "     # Setting up the driver\n",
    "    try:\n",
    "        driver.get(\"https://www.allrecipes.com/recipes-a-z-6735880\")\n",
    "        driver.maximize_window()   \n",
    "    except:\n",
    "        print(\"Failed to load the topics page\")\n",
    "        return\n",
    "    \n",
    "    # Getting the topics from the index page\n",
    "    html_content = driver.page_source\n",
    "    soup = BeautifulSoup(html_content,'html.parser')\n",
    "    links_element = soup.findAll(\"a\", {\"class\" : \"link-list__link type--dog-bold type--dog-link\"})\n",
    "    links_list = []\n",
    "    for element in links_element:\n",
    "        links_list.append(element[\"href\"])\n",
    "\n",
    "    return links_list\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activating our functions.</br>\n",
    "Scrape the index page for topics and add them to a list (output of TopicPageScrape)</br>\n",
    "then for each topic scrape its page and add all the recipes list (output of MultipleRecipesPageScrape) to a new list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--headless')\n",
    "service = Service('C:\\SeleniumDrivers')\n",
    "driver = webdriver.Chrome(service = service, options = options)\n",
    "\n",
    "topic_list = TopicPageScrape(driver)\n",
    "recipe_list_of_lists = []\n",
    "for topic in topic_list:\n",
    "    recipe_list_of_lists.append(MultipleRecipesPageScrape(topic, driver))\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each list of recipes in the recipe_list_of_lists, activate SingleRecipePageScrape, and add them to the DataFrame's list of values.</br> and create a DataFrame from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = []\n",
    "for recipe_list in recipe_list_of_lists:\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('--headless')\n",
    "    service = Service('C:\\SeleniumDrivers')\n",
    "    driver = webdriver.Chrome(service = service, options = options)\n",
    "    for recipe in recipe_list:\n",
    "        try:\n",
    "            data_list.append(SingleRecipePageScrape(recipe, driver))\n",
    "        except:\n",
    "            continue\n",
    "    driver.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing None Rows, we may get if the scraping fails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lst in data_list:\n",
    "    if lst == None:\n",
    "        data_list.remove(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe_df = pd.DataFrame(data_list)\n",
    "recipe_df.columns = [\"Recipe\", \"Date\", \"Rating\", \"Number of Raters\", \"Time\", \"Categories\", \"Servings\", \"Ingredients\", \"Instructions\", \"Calories\", \"Fat\", \"Carbs\",\"Proteins\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe_df.to_csv('RecipeData.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
